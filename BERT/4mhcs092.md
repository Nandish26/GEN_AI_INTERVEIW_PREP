1) What is BERT and why was it introduced?

BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained deep learning model for Natural Language Processing (NLP) introduced by Google in 2018.

It was introduced to:

Understand context from both left and right of a word.

Improve performance on tasks like sentiment analysis, question answering, and NER.

Overcome limitations of traditional left-to-right language models.

2) What does ‚ÄúBidirectional‚Äù mean in BERT?

Bidirectional means BERT reads text from both directions simultaneously.

Example:
‚ÄúI went to the bank to deposit money.‚Äù
BERT understands bank by looking at both left context (‚Äúwent to‚Äù) and right context (‚Äúdeposit money‚Äù).

Earlier models processed text only left‚Üíright or right‚Üíleft.

3) Who developed BERT and in which year?

BERT was developed by Google in 2018.

It was introduced in the research paper:
‚ÄúBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.‚Äù

4) What problem in traditional NLP models does BERT solve?

Traditional models:

Used static word embeddings (Word2Vec, GloVe).

Could not understand contextual meaning.

Were unidirectional (left-to-right).

BERT solves:

Context understanding.

Polysemy problem (same word, different meanings).

Transfer learning in NLP.

5) What are the main components of BERT architecture?

BERT is built using:

Transformer Encoder layers

Multi-head self-attention

Feed-forward neural networks

Positional encoding

Layer normalization

It does not use decoder layers.

6) Explain the Transformer architecture used in BERT.

BERT uses only the Encoder part of the Transformer architecture introduced in ‚ÄúAttention Is All You Need‚Äù by Google Brain.

Main parts:

Multi-head self-attention

Add & Norm

Feed-forward network

Residual connections

Self-attention allows each word to attend to every other word.

7) Difference between BERT Base and BERT Large?
Feature	BERT Base	BERT Large
Layers	12	24
Hidden size	768	1024
Attention heads	12	16
Parameters	110M	340M

BERT Large is more accurate but requires more computation.

8) What are the two pre-training objectives of BERT?
1Ô∏è‚É£ Masked Language Modeling (MLM)

Randomly masks 15% of words and predicts them.

Example:
‚ÄúThe sky is [MASK].‚Äù ‚Üí Predict ‚Äúblue‚Äù

2Ô∏è‚É£ Next Sentence Prediction (NSP)

Predicts whether sentence B follows sentence A.

9) How does Masked Language Modeling work?

15% of tokens are selected.

80% replaced with [MASK]

10% replaced with random word

10% unchanged

The model predicts the original word.

This helps BERT learn deep contextual representations.

10) Why does BERT use WordPiece tokenization?

WordPiece:

Breaks rare words into subwords.

Handles unknown words.

Reduces vocabulary size.

Example:
‚Äúplaying‚Äù ‚Üí ‚Äúplay‚Äù + ‚Äú##ing‚Äù

11) How is BERT fine-tuned for downstream tasks?

Steps:

Load pre-trained BERT.

Add task-specific output layer.

Train on labeled dataset.

Examples:

Sentiment analysis ‚Üí Add classification layer.

Question answering ‚Üí Predict start & end positions.

Fine-tuning is fast compared to training from scratch.

12) Difference between BERT and GPT?

GPT was developed by OpenAI.

Feature	BERT	GPT
Architecture	Encoder	Decoder
Direction	Bidirectional	Left-to-right
Main task	Understanding	Text generation
Training objective	MLM + NSP	Next word prediction

BERT ‚Üí Best for understanding
GPT ‚Üí Best for generation

13) Limitations of BERT

High computational cost

Large memory usage

Maximum input length 512 tokens

Not ideal for text generation

Slow inference compared to smaller models

14) How does attention mechanism work in BERT?

Self-attention:

Each word generates Query (Q), Key (K), Value (V).

Attention score = similarity(Q, K)

Weighted sum of V gives final output.

Formula:
Attention(Q, K, V) = softmax(QK·µÄ / ‚àöd‚Çñ) V

This allows capturing long-range dependencies.

15) Variants of BERT and differences
üîπ RoBERTa

Developed by Meta AI

Removes NSP

Trained longer

Better performance

üîπ DistilBERT

Smaller and faster version

40% smaller, 60% faster

Slightly less accurate

üîπ ALBERT

Parameter sharing

Factorized embeddings

Much fewer parameters